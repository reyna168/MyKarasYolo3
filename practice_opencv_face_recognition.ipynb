{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###=============== KNN start =======================\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\"C:/Users/reyna/Anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_alt.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 0\n",
    "face_data = []\n",
    "dataset_path = 'opencv_practice/'\n",
    "offset = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the name of the person :  anna\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8197404056b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#extract main face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mface_section\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mface_section\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mface_section\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "file_name = input(\"Enter the name of the person :  \")\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    if(ret == False):\n",
    "        continue\n",
    "    gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)    \n",
    "    faces = face_cascade.detectMultiScale(frame,1.3,5)\n",
    "    #print(faces)\n",
    "    faces = sorted(faces,key=lambda f:f[2]*f[3])\n",
    "    \n",
    "    #pick the last face (largest)\n",
    "    for face in faces[-1:]:\n",
    "        x,y,w,h =  face \n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "    #extract main face\n",
    "    face_section = frame[y-offset:y+h+offset,x-offset:x+w+offset]\n",
    "    face_section = cv2.resize(face_section,(100,100))\n",
    "    \n",
    "    skip += 1\n",
    "    if(skip%10 == 0):\n",
    "        face_data.append(face_section)\n",
    "        print(len(face_data))\n",
    "         \n",
    "    cv2.imshow(\"Cropped\",face_section) \n",
    "    cv2.imshow(\"VIDEO FRAME\",frame)\n",
    "        \n",
    "    keypressed = cv2.waitKey(1) & 0xFF\n",
    "    if(keypressed == ord('q')):\n",
    "        break\n",
    "    if keypressed == ord(' '):\n",
    "        cv2.imwrite('opencv_practice/cap_face.jpg',frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our face list array into a numpy array\n",
    "face_data = np.array(face_data)\n",
    "print(face_data.shape)\n",
    "face_data = face_data.reshape((face_data.shape[0],-1))\n",
    "print(face_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this data into file system\n",
    "np.save(dataset_path + file_name + '.npy',face_data)\n",
    "print(\"data successfully saved at \" + dataset_path+file_name+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building The Face Classifier\n",
    "# Recognise Faces using the classification algorithm — KNN.\n",
    "# 1. load the training data (numpy arrays of all the persons)\n",
    "# x- values are stored in the numpy arrays\n",
    "# y-values we need to assign for each person\n",
    "# 2. Read a video stream using opencv\n",
    "# 3. extract faces out of it\n",
    "# 4. use knn to find the prediction of face (int)\n",
    "# 5. map the predicted id to name of the user\n",
    "# 6. Display the predictions on the screen — bounding box and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KNN CODE ############\n",
    "def distance(v1, v2):\n",
    "    # Eucledian\n",
    "    return np.sqrt(((v1-v2)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(train, test, k=5):\n",
    "    dist = []\n",
    "    \n",
    "    for i in range(train.shape[0]):\n",
    "        # Get the vector and label\n",
    "        ix = train[i, :-1]\n",
    "        iy = train[i, -1]\n",
    "        # Compute the distance from test point\n",
    "        d = distance(test, ix)\n",
    "        dist.append([d, iy])\n",
    "    # Sort based on distance and get top k\n",
    "    dk = sorted(dist, key=lambda x: x[0])[:k]\n",
    "    # Retrieve only the labels\n",
    "    labels = np.array(dk)[:, -1]\n",
    "    \n",
    "    # Get frequencies of each label\n",
    "    output = np.unique(labels, return_counts=True)\n",
    "    # Find max frequency and corresponding label\n",
    "    index = np.argmax(output[1])\n",
    "    return output[0][index]\n",
    "########### KNN CODE ##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init Camera\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Detection\n",
    "face_cascade = cv2.CascadeClassifier(\".conda/envs/tensorflow2/lib/site-packages/cv2/data/haarcascade_frontalface_alt.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip = 0\n",
    "dataset_path = 'opencv_practice/'\n",
    "\n",
    "face_data = []\n",
    "labels = []\n",
    "\n",
    "class_id = 0 # Labels for the given file\n",
    "names = {} #Mapping btw id - name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dcd65c32e143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Data Preparation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mfx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#Create a mapping btw class_id and name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loaded '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "for fx in os.listdir(dataset_path):\n",
    "    if fx.endswith('.npy'): #Create a mapping btw class_id and name\n",
    "        names[class_id] = fx[:-4]\n",
    "        print('Loaded '+fx)\n",
    "        data_item = np.load(dataset_path+fx)\n",
    "        face_data.append(data_item)\n",
    "        #Create Labels for the class\n",
    "        target = class_id*np.ones((data_item.shape[0],))\n",
    "        class_id += 1\n",
    "        labels.append(target)\n",
    "\n",
    "face_dataset = np.concatenate(face_data,axis=0)\n",
    "face_labels = np.concatenate(labels,axis=0).reshape((-1,1))\n",
    "\n",
    "print(face_dataset.shape)\n",
    "print(face_labels.shape)\n",
    "\n",
    "trainset = np.concatenate((face_dataset,face_labels),axis=1)\n",
    "print(trainset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e74f2302462c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_cascade\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "# Testing \n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if ret == False:\n",
    "        continue\n",
    "\n",
    "    faces = face_cascade.detectMultiScale(frame,1.3,5)\n",
    "    if(len(faces)==0):\n",
    "        continue\n",
    "\n",
    "    for face in faces:\n",
    "        x,y,w,h = face\n",
    "\n",
    "        #Get the face ROI\n",
    "        offset = 10\n",
    "        face_section = frame[y-offset:y+h+offset,x-offset:x+w+offset]\n",
    "        face_section = cv2.resize(face_section,(100,100))\n",
    "\n",
    "        #Predicted Label (out)\n",
    "        out = knn(trainset,face_section.flatten())\n",
    "\n",
    "        #Display on the screen the name and rectangle around it\n",
    "        pred_name = names[int(out)]\n",
    "        cv2.putText(frame,pred_name,(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2,cv2.LINE_AA)\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "\n",
    "    cv2.imshow(\"Faces\",frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key==ord('q'):\n",
    "        break\n",
    "    if key == ord('s'):\n",
    "        cv2.imwrite('opencv_practice/cap_face_reg.jpg',frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#==================== KNN End ========================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= SVM Start ===================\n",
    "# pip install sklearn\n",
    "import os\n",
    "import pdb\n",
    "import glob\n",
    "import scipy.misc \n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn import svm,metrics \n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "#from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "#import config\n",
    "import math\n",
    "import imageio\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_CASCADE_FILE = \".conda/envs/tensorflow2/lib/site-packages/cv2/data/haarcascade_frontalface_alt.xml\"\n",
    "EYE_CASCADE_FILE = \".conda/envs/tensorflow2/lib/site-packages/cv2/data/haarcascade_eye.xml\"\n",
    "DEFAULT_FACE_SIZE = (200, 200)\n",
    "RECOGNIZER_OUTPUT_FILE = \"opencv_practice\\train_result.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a557c3b234ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mconvert_to_grey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fr-svm/orl_faces\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mconvert_to_grey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fr-svm/test-data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-a557c3b234ff>\u001b[0m in \u001b[0;36mconvert_to_grey\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# convert input image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_grey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfiles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"*/*\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "# convert input image\n",
    "def convert_to_grey(path):\n",
    "    files = glob.glob(os.path.join(path,\"*/*\"))\n",
    "\n",
    "    for f in files:\n",
    "        img = cv2.imread(f)\n",
    "        img = cv2.cvtColor( img, cv2.COLOR_RGB2GRAY )\n",
    "        str_ing = []\n",
    "        if(f.find(\".jpg\")>=0):\n",
    "            str_ing = f.split(\".jpg\")\n",
    "            final = str_ing[0] + \".jpg\"\n",
    "            cv2.imwrite( final, img )\n",
    "            print (final)\n",
    "        if(f.find(\".png\")>=0):\n",
    "            str_ing = f.split(\".png\")\n",
    "            final = str_ing[0] + \".png\"\n",
    "            cv2.imwrite( final, img )\n",
    "            print (final)\n",
    "        if(f.find(\".jpeg\")>=0):\n",
    "            str_ing = f.split(\".jpeg\")\n",
    "            final = str_ing[0] + \".jpeg\"\n",
    "            cv2.imwrite( final, img )\n",
    "            print( final )\n",
    "        if(f.find(\".pgm\")>=0):\n",
    "            continue\n",
    "            \n",
    "convert_to_grey(\"Fr-svm/orl_faces\")\n",
    "convert_to_grey(\"Fr-svm/test-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FACE_CASCADE_FILE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7a0e491b94b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mface_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFACE_CASCADE_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0meye_cascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEYE_CASCADE_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FACE_CASCADE_FILE' is not defined"
     ]
    }
   ],
   "source": [
    "#crop_faces\n",
    "import os.path\n",
    "\n",
    "cnt = 0\n",
    "face_cascade = cv2.CascadeClassifier(FACE_CASCADE_FILE)\n",
    "eye_cascade = cv2.CascadeClassifier(EYE_CASCADE_FILE)\n",
    "\n",
    "DIR = 'Fr-svm/orl_faces'\n",
    "numPics = len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))])\n",
    "\n",
    "pic = 1\n",
    "files = glob.glob(os.path.join(DIR,\"*/*\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fr-svm/orl_faces\\AamairKhan\\1.jpeg\n",
      "Image1 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AamairKhan\\10-Reasons-Why-Aamir-Khan-Is-The-Real-Perfectionist-Of-Bollywood-9.jpg\n",
      "Image2 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AamairKhan\\1509611919.jpg\n",
      "Image3 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AamairKhan\\17-Aamir-Khan.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\533650158.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\533650216.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\5a7f2ad5a7e37.image.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\817460.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\aamir-khan-2-091214.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\aamir-khan-20160704165255-2360.jpg\n",
      "Image10 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AamairKhan\\Aamir_Khan_2013.jpg\n",
      "Image11 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AamairKhan\\dangal-aamir-khan-instagram_640x480_51499859868.jpg\n",
      "Fr-svm/orl_faces\\AamairKhan\\images (1).jpeg\n",
      "Fr-svm/orl_faces\\AamairKhan\\images (2).jpeg\n",
      "Fr-svm/orl_faces\\AamairKhan\\images.jpeg\n",
      "Image15 has been processed and cropped\n",
      "Fr-svm/orl_faces\\abhishek\\2016-08-26-15-11-13-539.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\2016-09-04-16-58-14-588.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\2016-09-16-16-52-52-131.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\2016-09-30-20-40-52-817.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\2016-10-08-21-39-54-858.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\2016-10-20-20-21-30-847.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\2016-10-20-20-22-49-978.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\IMG-20160824-WA0003.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\IMG-20161002-WA0002.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\IMG-20161119-WA0016.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\IMG_20170415_192722.jpg\n",
      "Fr-svm/orl_faces\\abhishek\\IMG_20170424_170835.jpg\n",
      "Fr-svm/orl_faces\\AkshayKumar\\20180119102825-oneAK.jpeg\n",
      "Image28 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\60435616 (1).jpg\n",
      "Image29 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\Akshay Kumar.jpg\n",
      "Fr-svm/orl_faces\\AkshayKumar\\akshay-kumar-20151013144728-78.jpg\n",
      "Fr-svm/orl_faces\\AkshayKumar\\akshay-kumar-94-20-12-2017-04-36-47.jpg\n",
      "Image32 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\akshay-kumar-instagra_650x400_71511586805.jpg\n",
      "Image33 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\Akshay-Kumar.jpg\n",
      "Image34 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\Akshay_Kumar (1).jpg\n",
      "Image35 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\Akshay_Kumar.jpg\n",
      "Fr-svm/orl_faces\\AkshayKumar\\dc-Cover-62k5vk51v73ltdp8nq8nbjc674-20170427093849.Medi.jpeg\n",
      "Image37 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\images.jpeg\n",
      "Image38 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\main-qimg-958e5917f9d5e73d246faa6d40935c0d-c.jpeg\n",
      "Image39 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AkshayKumar\\_3cae9032-281e-11e7-a4a0-8e0501b9fa54.jpeg\n",
      "Image40 has been processed and cropped\n",
      "Fr-svm/orl_faces\\AliaBhatt\\1.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\10.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\11.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\12.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\13.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\14.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\15.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\16.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\17.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\18.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\19.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\2.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\20.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\3.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\4.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\5.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\6.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\7.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\8.jpg\n",
      "Fr-svm/orl_faces\\AliaBhatt\\9.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\1.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\10.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\11.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\12.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\13.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\14.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\15.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\16.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\17.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\18.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\19.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\2.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\20.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\3.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\4.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\5.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\6.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\7.jpg\n",
      "Fr-svm/orl_faces\\sharukhKhan\\8.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\1.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\10.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\11.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\12.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\13.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\14.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\15.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\16.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\17.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\18.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\2.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\20.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\3.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\4.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\5.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\6.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\7.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\8.jpg\n",
      "Fr-svm/orl_faces\\siddhartmalhotra\\9.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\1.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\10.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\11.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\12.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\13.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\14.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\15.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\16.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\17.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\18.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\19.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\2.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\20.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\3.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\5.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\6.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\7.jpg\n",
      "Fr-svm/orl_faces\\varundhawan\\8.jpg\n",
      "All images have been processed!!!\n"
     ]
    }
   ],
   "source": [
    "for f in files:\n",
    "    \n",
    "    img = cv2.imread(f)\n",
    "    print (f)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    size = height * width\n",
    "\n",
    "    if size > (500^2):\n",
    "        r = 500.0 / img.shape[1]\n",
    "        dim = (500, int(img.shape[0] * r))\n",
    "        img2 = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        img = img2\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    cas_rejectLevel = 1.3\n",
    "    cas_levelWeight = 5\n",
    "\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        eyesn = 0\n",
    "        imgCrop = img[y:y+h,x:x+w]\n",
    "        #cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            #cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "            eyesn = eyesn +1\n",
    "        if eyesn >= 2:\n",
    "            #### increase the counter and save \n",
    "            cnt +=1\n",
    "            file_name = f.split(\"/\")[-1] \n",
    "            cv2.imwrite(f, imgCrop)\n",
    "\n",
    "            #cv2.imshow('img',imgCrop)\n",
    "            print(\"Image\"+str(pic)+\" has been processed and cropped\")\n",
    "\n",
    "    pic = pic+1\n",
    "    k = cv2.waitKey(100) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "#cap.release()\n",
    "print(\"All images have been processed!!!\")\n",
    "cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images have been processed!!!\n"
     ]
    }
   ],
   "source": [
    "# multiple faces \n",
    "dictionary = {}\n",
    "\n",
    "for f in files:\n",
    "    \n",
    "    img = cv2.imread(f)\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "    size = height * width\n",
    "    temp_img = img\n",
    "    if size > (500^2):\n",
    "        r = 500.0 / img.shape[1]\n",
    "        dim = (500, int(img.shape[0] * r))\n",
    "        img2 = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        img = img2\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    cas_rejectLevel = 1.3\n",
    "    cas_levelWeight = 5\n",
    "    pic = 1\n",
    "    for l in faces:\n",
    "        #print l\n",
    "        eyesn = 0\n",
    "        x = l[0]\n",
    "        y = l[1]\n",
    "        w = l[2]\n",
    "        h = l[3]\n",
    "        temp_list = (x,y,w,h)\n",
    "        imgCrop = img[y:y+h,x:x+w]\n",
    "        cv2.rectangle(temp_img,(x,y),(x+w,y+h),(255,0,0),3)\n",
    "        cv2.putText(temp_img,\" face\", (x, y), cv2.FONT_HERSHEY_PLAIN, 2, (0,0,255), 2, cv2.LINE_AA)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = img[y:y+h, x:x+w]\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            eyesn = eyesn +1\n",
    "        if eyesn >= 2:\n",
    "            #### increase the counter and save \n",
    "            file_name = f.split(\"/\")[-1]\n",
    "            check = file_name.split(\".\")[0]\n",
    "            check_formaat = file_name.split(\".\")[-1]\n",
    "            ans = \"opencv_practice/output/data/\" + check + \"_\"+ str(pic)+\".\" +check_formaat\n",
    "            cv2.imwrite(ans, imgCrop)\n",
    "            temp = \"opencv_practice/output/data/\" + check +\"_\"+ str(pic)+ \".\"+ check_formaat\n",
    "            pic = pic+1\n",
    "            temp_dict = {temp:temp_list}\n",
    "            dictionary.update(temp_dict)\n",
    "            cv2.imshow('img',temp_img)\n",
    "            #print(\"Image\"+str(pic)+\" has been processed and cropped\")\n",
    "            if cv2.waitKey(200) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "\n",
    "#cap.release()\n",
    "pickle.dump(dictionary, open(\"opencv_practice/dictionary.txt\", 'wb'))\n",
    "print(\"All images have been processed!!!\")\n",
    "cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary\n",
    "check = {\"test30.jpg\":\"abhishek\",\"test29.jpg\":\"AamairKhan\",\"test28.jpg\":\"AamairKhan\",\"test26.jpg\":\"AkshayKumar\",\"test25.jpg\":\"AamairKhan\",\"test27.jpg\":\"AkshayKumar\",\"1.jpg\":\"siddhartmalhotra\",\"2.jpg\":\"AliaBhatt\",\"6.jpg\":\"varundhawan\",\"test7.jpg\":\"sharukhkhanAliaBhatt\",\"test24.jpg\":\"AliaBhattsiddhartmalhotra\",\"test29.jpeg\":\"AkshayKumar\",\"test30.jpeg\":\"AkshayKumar\",\"test31.jpeg\":\"AamairKhan\",\"test32.jpg\":\"AamairKhan\",\"test33.jpeg\":\"sharukhkhan\",\"test34.jpeg\":\"sharukhkhan\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    files = glob.glob(os.path.join(path,\"*/*\"))\n",
    "    #print files\n",
    "    for f in files:\n",
    "        #img = scipy.misc.imread(f)\n",
    "        img = imageio.imread(f)\n",
    "        bc = resize(img, (112, 92))\n",
    "        result = bc\n",
    "        if(np.array(bc).ndim==3):\n",
    "            result = bc[:,:,0]\n",
    "        images.append(result)\n",
    "        #images.append(bc)\n",
    "        file_name = f.split(\"/\")[-2]\n",
    "        labels.append(file_name)\n",
    "    #print(len(images))\n",
    "    return np.array(images).reshape((len(images),-1)),np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    files = glob.glob(os.path.join(path,\"*/*\"))\n",
    "    files.sort()\n",
    "    for f in files:\n",
    "        #img = scipy.misc.imread(f)\n",
    "        img = imageio.imread(f)\n",
    "        bc = resize(img, (112, 92))\n",
    "        result = bc[:,:,0]\n",
    "        images.append(result)\n",
    "        file_name = f.split(\"/\")[-1]\n",
    "        labels.append(file_name)\n",
    "    #print(image.shape)\n",
    "    return np.array(images).reshape((len(images),-1)),np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runs for testing only\n",
    "#========================================================================================\n",
    "\n",
    "\n",
    "def test_it(model,images,valid_labels,verbose=True):\n",
    "    prediction = model.predict(images)\n",
    "    if verbose:\n",
    "        pretty_print_it(prediction,valid_labels)\n",
    "    return prediction\n",
    "\n",
    "def pretty_print_it(prediction,valid_labels):\n",
    "    assert prediction is not None,\"Prediction was not performed\"\n",
    "    print (\"==============================================\")\n",
    "    #print metrics.classification_report(valid_labels,prediction)\n",
    "    print (\"==============================================\")\n",
    "\n",
    "def calculate_accuracy(test_labels,prediction,dictionary):\n",
    "    overall_accuracy = 0.0\n",
    "    total_img = len(prediction)\n",
    "    test_path = \"opencv_practice/test-data/data/\"\n",
    "    count = 0\n",
    "    for img in test_labels:\n",
    "        name = prediction[count]\n",
    "        count = count + 1\n",
    "        crop_test_image_path = \"opencv_practice/output/data/\"+img\n",
    "        x,y,w,h = dictionary[crop_test_image_path]\n",
    "        ans = img.split(\"_\")[0]\n",
    "        ans2 = img.split(\".\")[-1]\n",
    "        img =ans + \".\"+ans2\n",
    "        if ((check[img] == name) or (check[img].find(name)>=0)) :\n",
    "            overall_accuracy = overall_accuracy +1\n",
    "        image_path = test_path  + ans + \".\"+ans2\n",
    "        actual_image = cv2.imread(image_path)\n",
    "        print (image_path)\n",
    "        cv2.putText(actual_image,name, (x, y), cv2.FONT_HERSHEY_PLAIN, 1, (0,0,255), 1, cv2.LINE_AA)\n",
    "        #cv2.putText(actual_image,name,(10, 50),cv2.FONT_HERSHEY_PLAIN, 1, (0,255,0), 1, cv2.LINE_AA)\n",
    "        cv2.imshow(\"camera\", actual_image)\n",
    "        if cv2.waitKey(800) & 0xFF == ord('q'):\n",
    "            break\n",
    "    overall_accuracy = (overall_accuracy/total_img)*100.0\n",
    "    print (\"The overall accuracy is :\"  +  str(overall_accuracy))\n",
    "\n",
    "#=======================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded images and labels of shape (116, 10304) and (116,)\n"
     ]
    }
   ],
   "source": [
    "# initialize SVM\n",
    "gamma=0.001\n",
    "images,labels = load_data(\"Fr-svm/orl_faces\")\n",
    "classifier = svm.LinearSVC(verbose=1)\n",
    "length = images.shape[0]\n",
    "print ('Loaded images and labels of shape %s and %s' % (images.shape,labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split as tts\n",
    "# train\n",
    "def train():\n",
    "    print (\"Starting Training\")\n",
    "\n",
    "    #rs = StratifiedShuffleSplit(length,n_iter=5,test_size=0.25,random_state=None)\n",
    "    #rs = ShuffleSplit(n_splits=10,test_size=0.25,random_state=0)\n",
    "    rs = StratifiedShuffleSplit(n_splits=10,test_size=0.25,random_state=0)\n",
    "    rs.get_n_splits(images,labels)\n",
    "    #print (rs)\n",
    "    fold = 1\n",
    "    #x_train, x_test, income_train, income_test = tts( other_colums, income_column, shuffle = True, stratify = Income_column)\n",
    "    for train_index,test_index in rs.split(images, labels):\n",
    "        train_images,train_labels = images[train_index],labels[train_index]\n",
    "        valid_images,valid_labels = images[test_index],labels[test_index]\n",
    "        print (train_images)\n",
    "        svm_classifier = classifier.fit(train_images,train_labels)\n",
    "        # save the model to disk\n",
    "        fold+=1\n",
    "\n",
    "    filename = 'opencv_practice\\finalized_model.sav'\n",
    "    pickle.dump(svm_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training\n",
      "[[0.52156863 0.5254902  0.49456522 ... 0.36078431 0.36078431 0.36965047]\n",
      " [0.10540386 0.09803922 0.09184174 ... 0.07095055 0.0745098  0.0745098 ]\n",
      " [0.10813847 0.07852157 0.02297528 ... 0.75733924 0.75268885 0.69769478]\n",
      " ...\n",
      " [0.02093838 0.02352941 0.03017903 ... 0.10510596 0.11602424 0.24466569]\n",
      " [0.07414292 0.02455738 0.0272546  ... 0.50196078 0.49920191 0.49411765]\n",
      " [0.47058824 0.46092837 0.44606625 ... 0.96394319 0.95629415 0.74104897]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'Fr-svm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-f4c7506fd69a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mcalculate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-21837bb378be>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mvalid_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalid_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msvm_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# save the model to disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mfold\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow2\\lib\\site-packages\\sklearn\\svm\\classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"crammer_singer\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorflow2\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[1;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[0;32m    877\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0;32m    878\u001b[0m                              \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 879\u001b[1;33m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m         \u001b[0mclass_weight_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'Fr-svm'"
     ]
    }
   ],
   "source": [
    "#Start SVM\n",
    "foldername = 'Fr-svm\\orl_faces'\n",
    "fold=5\n",
    "test_size=0.25\n",
    "random_state= None\n",
    "glob_searh='*/*'\n",
    "verbose=1\n",
    "model_file= 'opencv_practice\\finalized_model.sav'\n",
    "dictionary={}\n",
    "if os.path.isfile(model_file):\n",
    "    print (\"model is already present\")\n",
    "    with open(\"Fr-svm\\dictionary.txt\",\"rb\") as fp:\n",
    "        dictionary = pickle.load(fp, encoding='iso-8859-1')\n",
    "        with open(\"opencv_practice\\finalized_model.sav\",\"rb\") as fp:\n",
    "            model = pickle.load(fp, encoding='iso-8859-1')\n",
    "            test_images,test_labels = load_data_test(\"output\")\n",
    "            prediction  = test_it(model,test_images,test_labels)\n",
    "            print ((prediction))\n",
    "            print ((test_labels))\n",
    "            calculate_accuracy(test_labels,prediction,dictionary)\n",
    "else:\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
